{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration\n",
    "## Ticket Classification Dataset Analysis\n",
    "\n",
    "**Objective**: Explore the structure and basic characteristics of the ticket dataset\n",
    "- Dataset: 6,968 tickets with extensive text fields\n",
    "- Challenge: Multi-line CSV due to extensive Comments/Work Notes and Description fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n",
      "File size: 42.4 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# File path - use absolute path\n",
    "data_path = Path('/Users/sparshk/Documents/Development/Ticket Classification/data/raw/actual_raw_data.csv')\n",
    "print(f\"File exists: {data_path.exists()}\")\n",
    "if data_path.exists():\n",
    "    print(f\"File size: {data_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"ERROR: File not found! Please check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initial File Examination\n",
    "Let's first understand the file structure and handle the multi-line CSV properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 lines of the file:\n",
      " 1: ﻿Short description,Category,Priority,Assignment group,Subcategory,Vendor,Comments and Work notes,Des...\n",
      " 2: TILL: Tills are down,Server,1 - Critical,WKS-HEX-Server-Support,Windows Server,JBrand,\"2025-01-24 19...\n",
      " 3: Incident automatically closed by system after 7 days in the Resolved state.\n",
      " 4: \n",
      " 5: 2025-01-17 18:20:21 - Vinoth Kumar (Work notes)\n",
      " 6: Knowledge article[code]<a href=kb_view.do?sys_kb_id=ac0bd5bd9790c2501049b0d3f153af0c>KB0010949</a>[/...\n",
      " 7: Version: 1\n",
      " 8: \n",
      " 9: 2025-01-17 16:22:15 - Kuppanagari Maghana (Additional comments)\n",
      "10: Hi Cindy,\n"
     ]
    }
   ],
   "source": [
    "# First, let's examine the first few lines to understand structure\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    first_lines = [f.readline().strip() for _ in range(10)]\n",
    "    \n",
    "print(\"First 10 lines of the file:\")\n",
    "for i, line in enumerate(first_lines, 1):\n",
    "    print(f\"{i:2d}: {line[:100]}{'...' if len(line) > 100 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines in file: 519,107\n",
      "Expected records if ~6,968 tickets: 6968\n",
      "Average lines per record: 74.5\n"
     ]
    }
   ],
   "source": [
    "# Count total lines in file\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f)\n",
    "\n",
    "print(f\"Total lines in file: {total_lines:,}\")\n",
    "print(f\"Expected records if ~6,968 tickets: {6968}\")\n",
    "print(f\"Average lines per record: {total_lines / 6968:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data with Proper CSV Handling\n",
    "Handle the multi-line nature of the CSV due to extensive text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded DataFrame with shape: (6968, 8)\n",
      "Columns: ['Short description', 'Category', 'Priority', 'Assignment group', 'Subcategory', 'Vendor', 'Comments and Work notes', 'Description']\n"
     ]
    }
   ],
   "source": [
    "# Try loading with different CSV parameters to handle multi-line fields\n",
    "try:\n",
    "    # Load with quoting to handle multi-line fields properly\n",
    "    df = pd.read_csv(data_path, \n",
    "                     encoding='utf-8',\n",
    "                     quoting=1,  # QUOTE_ALL\n",
    "                     skipinitialspace=True,\n",
    "                     on_bad_lines='warn')\n",
    "    \n",
    "    print(f\"Successfully loaded DataFrame with shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading with standard method: {e}\")\n",
    "    print(\"Trying alternative loading method...\")\n",
    "    \n",
    "    # Alternative: try with different parameters\n",
    "    df = pd.read_csv(data_path, \n",
    "                     encoding='utf-8',\n",
    "                     quotechar='\"',\n",
    "                     skipinitialspace=True,\n",
    "                     on_bad_lines='skip')\n",
    "    \n",
    "    print(f\"Loaded with alternative method. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET OVERVIEW ===\n",
      "Shape: (6968, 8)\n",
      "Columns: 8\n",
      "\n",
      "Column names:\n",
      " 1. Short description\n",
      " 2. Category\n",
      " 3. Priority\n",
      " 4. Assignment group\n",
      " 5. Subcategory\n",
      " 6. Vendor\n",
      " 7. Comments and Work notes\n",
      " 8. Description\n"
     ]
    }
   ],
   "source": [
    "# Basic info about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA TYPES ===\n",
      "Short description          object\n",
      "Category                   object\n",
      "Priority                   object\n",
      "Assignment group           object\n",
      "Subcategory                object\n",
      "Vendor                     object\n",
      "Comments and Work notes    object\n",
      "Description                object\n",
      "dtype: object\n",
      "\n",
      "=== MEMORY USAGE ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6968 entries, 0 to 6967\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Short description        6968 non-null   object\n",
      " 1   Category                 6968 non-null   object\n",
      " 2   Priority                 6968 non-null   object\n",
      " 3   Assignment group         6968 non-null   object\n",
      " 4   Subcategory              6968 non-null   object\n",
      " 5   Vendor                   508 non-null    object\n",
      " 6   Comments and Work notes  6968 non-null   object\n",
      " 7   Description              6964 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 56.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check data types and basic statistics\n",
    "print(\"=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n=== MEMORY USAGE ===\")\n",
    "print(df.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Examine Sample Records\n",
    "Look at a few complete records to understand the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST 3 RECORDS ===\n",
      "\n",
      "--- RECORD 1 ---\n",
      "Short description: TILL: Tills are down\n",
      "Category: Server\n",
      "Priority: 1 - Critical\n",
      "Assignment group: WKS-HEX-Server-Support\n",
      "Subcategory: Windows Server\n",
      "Vendor: JBrand\n",
      "Comments and Work notes: 2025-01-24 19:00:00 - System (Additional comments)\n",
      "Incident automatically closed by system after 7 days in the Resolved state.\n",
      "\n",
      "2025-01-17 18:20:21 - Vinoth Kumar (Work notes)\n",
      "Knowledge article[code]<...[TRUNCATED]\n",
      "Description: Description Of the Issue:  All tills are down, from the morning tills were working slow\n",
      "Impacted Location / Store:  8501\n",
      "Store Manger Name:  Cindy Skingley\n",
      "Manager Contact Number: +44 7852 953927\n",
      "What...[TRUNCATED]\n",
      "\n",
      "--- RECORD 2 ---\n",
      "Short description: Till/Backoffice/Zebra/ Phones - Down \n",
      "Category: Network\n",
      "Priority: 1 - Critical\n",
      "Assignment group: WKS-HEX-NWDATA-Support\n",
      "Subcategory: Network Down\n",
      "Vendor: [NULL]\n",
      "Comments and Work notes: 2025-01-20 14:00:01 - System (Additional comments)\n",
      "Incident automatically closed by system after 7 days in the Resolved state.\n",
      "\n",
      "2025-01-13 13:40:38 - Sathvik Dhanpal (Additional comments)\n",
      "The Zen ISP ...[TRUNCATED]\n",
      "Description: Description Of the Issue: Store have had no tills in online mode, back office, zebra. Tills have been rebooted before call so they are all stuck on blue screen store completely unable to serve custome...[TRUNCATED]\n",
      "\n",
      "--- RECORD 3 ---\n",
      "Short description: TILL: Tills are down\n",
      "Category: Server or Storage\n",
      "Priority: 1 - Critical\n",
      "Assignment group: WKS-HEX-Server-Support\n",
      "Subcategory: Server alerts\n",
      "Vendor: [NULL]\n",
      "Comments and Work notes: 2025-01-10 10:58:49 - Sheralee Kerr (Additional comments)\n",
      " Caller accepted \n",
      "\n",
      "2025-01-09 11:44:03 - Madhuri Cv (Work notes)\n",
      "Knowledge article[code]<a href=kb_view.do?sys_kb_id=ac0bd5bd9790c2501049b0d3f...[TRUNCATED]\n",
      "Description: Description Of the Issue: Transport error has occurred when receiving results from the server pcp provider error 0 \n",
      "Impacted Location / Store: 8338\n",
      "Store Manger Name:  Johannah Moret and Amie Hutching...[TRUNCATED]\n"
     ]
    }
   ],
   "source": [
    "# Display first few records\n",
    "print(\"=== FIRST 3 RECORDS ===\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n--- RECORD {i+1} ---\")\n",
    "    for col in df.columns:\n",
    "        value = str(df.iloc[i][col]) if pd.notna(df.iloc[i][col]) else \"[NULL]\"\n",
    "        # Truncate very long values\n",
    "        if len(value) > 200:\n",
    "            value = value[:200] + \"...[TRUNCATED]\"\n",
    "        print(f\"{col}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Basic Statistics and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MISSING VALUES ANALYSIS ===\n",
      "                                          Column  Missing Count  Missing %  \\\n",
      "Vendor                                    Vendor           6460      92.71   \n",
      "Description                          Description              4       0.06   \n",
      "Short description              Short description              0       0.00   \n",
      "Category                                Category              0       0.00   \n",
      "Priority                                Priority              0       0.00   \n",
      "Assignment group                Assignment group              0       0.00   \n",
      "Subcategory                          Subcategory              0       0.00   \n",
      "Comments and Work notes  Comments and Work notes              0       0.00   \n",
      "\n",
      "                        Data Type  \n",
      "Vendor                     object  \n",
      "Description                object  \n",
      "Short description          object  \n",
      "Category                   object  \n",
      "Priority                   object  \n",
      "Assignment group           object  \n",
      "Subcategory                object  \n",
      "Comments and Work notes    object  \n"
     ]
    }
   ],
   "source": [
    "# Missing values analysis\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': df.isnull().sum(),\n",
    "    'Missing %': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Data Type': df.dtypes\n",
    "})\n",
    "\n",
    "print(missing_stats.sort_values('Missing %', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEXT FIELD LENGTH ANALYSIS ===\n",
      "\n",
      "Comments and Work notes:\n",
      "  Min length: 127\n",
      "  Max length: 32,002\n",
      "  Mean length: 5848\n",
      "  Median length: 1510\n",
      "  95th percentile: 26520\n",
      "\n",
      "Description:\n",
      "  Min length: 3\n",
      "  Max length: 3,425\n",
      "  Mean length: 323\n",
      "  Median length: 272\n",
      "  95th percentile: 737\n",
      "\n",
      "Short description:\n",
      "  Min length: 4\n",
      "  Max length: 160\n",
      "  Mean length: 38\n",
      "  Median length: 33\n",
      "  95th percentile: 91\n"
     ]
    }
   ],
   "source": [
    "# Text field length analysis for the extensive fields\n",
    "print(\"=== TEXT FIELD LENGTH ANALYSIS ===\")\n",
    "\n",
    "text_fields = ['Comments and Work notes', 'Description', 'Short description']\n",
    "\n",
    "for field in text_fields:\n",
    "    if field in df.columns:\n",
    "        # Calculate lengths (handle NaN values)\n",
    "        lengths = df[field].astype(str).str.len()\n",
    "        \n",
    "        print(f\"\\n{field}:\")\n",
    "        print(f\"  Min length: {lengths.min():,}\")\n",
    "        print(f\"  Max length: {lengths.max():,}\")\n",
    "        print(f\"  Mean length: {lengths.mean():.0f}\")\n",
    "        print(f\"  Median length: {lengths.median():.0f}\")\n",
    "        print(f\"  95th percentile: {lengths.quantile(0.95):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initial Summary\n",
    "Summarize key findings from this initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIAL EXPLORATION SUMMARY ===\n",
      "✓ Dataset loaded successfully\n",
      "✓ Shape: 6,968 records × 8 columns\n",
      "✓ Expected ~6,968 tickets: ✓ MATCH\n",
      "✓ Memory usage: 56.1 MB\n",
      "\n",
      "=== NEXT STEPS ===\n",
      "1. Analyze structured columns (Category, Priority, Assignment group, etc.)\n",
      "2. Deep dive into extensive text fields\n",
      "3. Data quality assessment\n",
      "4. Distribution analysis\n",
      "5. Text preprocessing preparation\n"
     ]
    }
   ],
   "source": [
    "print(\"=== INITIAL EXPLORATION SUMMARY ===\")\n",
    "print(f\"✓ Dataset loaded successfully\")\n",
    "print(f\"✓ Shape: {df.shape[0]:,} records × {df.shape[1]} columns\")\n",
    "print(f\"✓ Expected ~6,968 tickets: {'✓ MATCH' if abs(df.shape[0] - 6968) < 100 else '✗ MISMATCH'}\")\n",
    "print(f\"✓ Memory usage: {df.memory_usage(deep=True).sum() / (1024*1024):.1f} MB\")\n",
    "\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Analyze structured columns (Category, Priority, Assignment group, etc.)\")\n",
    "print(\"2. Deep dive into extensive text fields\")\n",
    "print(\"3. Data quality assessment\")\n",
    "print(\"4. Distribution analysis\")\n",
    "print(\"5. Text preprocessing preparation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
