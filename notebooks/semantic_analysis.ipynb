{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Analysis - Phase 2: Unclassified Ticket Clustering\n",
    "\n",
    "This notebook processes the 3,208 tickets that couldn't be classified by hardcoded rules and uses sentence-transformers + clustering to find semantic groups representing core problem statements.\n",
    "\n",
    "## Process:\n",
    "1. Load unclassified tickets from improved classification results\n",
    "2. Encode ticket descriptions using sentence-transformers\n",
    "3. Apply DBSCAN clustering to find semantic groups\n",
    "4. Analyze clusters to identify core problem statements\n",
    "5. Generate final semantic taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries (Test Each One)\n",
    "\n",
    "Let's import libraries one by one to identify any issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic libraries imported successfully\n",
      "Pandas: 2.3.1\n",
      "NumPy: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries first\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "print(\"‚úÖ Basic libraries imported successfully\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Visualization libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Try matplotlib first (usually works)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ Visualization libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå HuggingFace transformers import failed: Could not import module 'AutoTokenizer'. Are this object's requirements defined correctly?\n",
      "‚ùå Scikit-learn not available\n"
     ]
    }
   ],
   "source": [
    "# Advanced Alternative: Use HuggingFace transformers directly (no scipy needed)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "    print(\"‚úÖ HuggingFace transformers available - can do advanced semantic analysis!\")\n",
    "    transformers_available = True\n",
    "    \n",
    "    # Custom cosine similarity using pure PyTorch/NumPy\n",
    "    def compute_cosine_similarity(embeddings):\n",
    "        \"\"\"Compute cosine similarity matrix using NumPy.\"\"\"\n",
    "        # Normalize embeddings\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        normalized_embeddings = embeddings / norms\n",
    "        # Compute cosine similarity\n",
    "        similarity_matrix = np.dot(normalized_embeddings, normalized_embeddings.T)\n",
    "        return similarity_matrix\n",
    "    \n",
    "    # Custom DBSCAN implementation\n",
    "    def custom_dbscan(distance_matrix, eps=0.3, min_samples=3):\n",
    "        \"\"\"Custom DBSCAN implementation without scipy.\"\"\"\n",
    "        n_points = distance_matrix.shape[0]\n",
    "        labels = np.full(n_points, -1)  # -1 means noise\n",
    "        cluster_id = 0\n",
    "        \n",
    "        for i in range(n_points):\n",
    "            if labels[i] != -1:  # Already processed\n",
    "                continue\n",
    "                \n",
    "            # Find neighbors within eps distance\n",
    "            neighbors = np.where(distance_matrix[i] <= eps)[0]\n",
    "            \n",
    "            if len(neighbors) < min_samples:\n",
    "                continue  # Point is noise\n",
    "                \n",
    "            # Start new cluster\n",
    "            labels[i] = cluster_id\n",
    "            seed_set = list(neighbors)\n",
    "            \n",
    "            j = 0\n",
    "            while j < len(seed_set):\n",
    "                point = seed_set[j]\n",
    "                \n",
    "                if labels[point] == -1:  # Change noise to border point\n",
    "                    labels[point] = cluster_id\n",
    "                elif labels[point] != -1:  # Already in cluster\n",
    "                    j += 1\n",
    "                    continue\n",
    "                    \n",
    "                labels[point] = cluster_id\n",
    "                \n",
    "                # Find new neighbors\n",
    "                new_neighbors = np.where(distance_matrix[point] <= eps)[0]\n",
    "                if len(new_neighbors) >= min_samples:\n",
    "                    for new_point in new_neighbors:\n",
    "                        if new_point not in seed_set:\n",
    "                            seed_set.append(new_point)\n",
    "                j += 1\n",
    "                \n",
    "            cluster_id += 1\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    print(\"‚úÖ Custom advanced clustering functions ready\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå HuggingFace transformers import failed: {e}\")\n",
    "    transformers_available = False\n",
    "\n",
    "# Try basic sklearn (might work even if scipy fails)\n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"‚úÖ Scikit-learn base imported\")\n",
    "    sklearn_base_available = True\n",
    "except ImportError:\n",
    "    sklearn_base_available = False\n",
    "    print(\"‚ùå Scikit-learn not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Sentence-transformers import failed: No module named 'scipy._cyutility'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sparshk/Documents/Development/Ticket Classification/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Try sentence-transformers (might fail here)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úÖ Sentence-transformers imported successfully\")\n",
    "    sentence_transformers_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Sentence-transformers import failed: {e}\")\n",
    "    sentence_transformers_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Simple Semantic Analysis (If Libraries Fail)\n",
    "\n",
    "If the advanced libraries fail, we can do semantic analysis using basic libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback: Simple similarity using basic Python\n",
    "def simple_jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def simple_text_similarity(text1, text2):\n",
    "    \"\"\"Simple text similarity using word overlap.\"\"\"\n",
    "    words1 = set(text1.lower().split())\n",
    "    words2 = set(text2.lower().split())\n",
    "    return simple_jaccard_similarity(words1, words2)\n",
    "\n",
    "print(\"‚úÖ Fallback similarity functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Unclassified Tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification results\n",
    "results_file = Path('../outputs/improved_classification_results.json')\n",
    "\n",
    "if not results_file.exists():\n",
    "    print(f\"‚ùå Error: Classification results file not found at {results_file}\")\n",
    "    print(\"   Please run improved_semantic_grouping_clean.py first.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found classification results file\")\n",
    "    \n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    unclassified_tickets = results['unclassified_tickets']\n",
    "    print(f\"üìä Loaded {len(unclassified_tickets)} unclassified tickets\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_unclassified = pd.DataFrame(unclassified_tickets)\n",
    "    print(f\"‚úÖ Created DataFrame with shape: {df_unclassified.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original ticket data to get full descriptions\n",
    "consolidated_file = Path('../data/processed/consolidated_tickets.csv')\n",
    "\n",
    "if consolidated_file.exists():\n",
    "    df_original = pd.read_csv(consolidated_file)\n",
    "    \n",
    "    # Merge to get full ticket information\n",
    "    df_unclassified = df_unclassified.merge(df_original, \n",
    "                     left_on='ticket_index', \n",
    "                     right_index=True, \n",
    "                     how='left',\n",
    "                     suffixes=('', '_orig'))\n",
    "    \n",
    "    print(f\"‚úÖ Successfully merged with original ticket data\")\n",
    "    print(f\"üìä Final DataFrame shape: {df_unclassified.shape}\")\n",
    "    print(f\"üìã Columns: {list(df_unclassified.columns)}\")\n",
    "else:\n",
    "    print(\"‚ùå Original ticket data not found - using limited information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show category breakdown\n",
    "print(\"üìã CATEGORY BREAKDOWN:\")\n",
    "category_counts = df_unclassified['category'].value_counts()\n",
    "for category, count in category_counts.head(10).items():\n",
    "    print(f\"   {category}: {count:,} tickets\")\n",
    "\n",
    "print(f\"\\nüìù SAMPLE TICKETS:\")\n",
    "for i, row in df_unclassified.head(5).iterrows():\n",
    "    print(f\"   {i+1}. [{row['category']}] '{row['short_description']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Text for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_analysis(row):\n",
    "    \"\"\"Prepare text for semantic analysis.\"\"\"\n",
    "    # Get short description and full description\n",
    "    short_desc = str(row.get('Short description', ''))\n",
    "    description = str(row.get('Description', ''))\n",
    "    \n",
    "    # Clean up\n",
    "    short_desc = short_desc.strip()\n",
    "    description = description.strip()\n",
    "    \n",
    "    # Strategy: Give more weight to short description (more focused)\n",
    "    # but include full description for context\n",
    "    if len(description) > 0 and description.lower() != 'nan':\n",
    "        # Combine with emphasis on short description\n",
    "        combined = f\"{short_desc}. {short_desc}. {description}\"\n",
    "    else:\n",
    "        # Only short description available\n",
    "        combined = f\"{short_desc}. {short_desc}.\"\n",
    "    \n",
    "    return combined.strip()\n",
    "\n",
    "# Prepare texts\n",
    "df_unclassified['prepared_text'] = df_unclassified.apply(prepare_text_for_analysis, axis=1)\n",
    "\n",
    "print(\"‚úÖ Text preparation complete\")\n",
    "print(f\"\\nüìù SAMPLE PREPARED TEXTS:\")\n",
    "for i, text in enumerate(df_unclassified['prepared_text'].head(3), 1):\n",
    "    print(f\"   {i}. {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4A: Advanced Semantic Analysis (If Libraries Work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformers_available:\n",
    "    print(\"üöÄ Using advanced semantic analysis with HuggingFace transformers\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    print(f\"Loading model: {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        def encode_texts(texts, batch_size=32):\n",
    "            \"\"\"Encode texts using HuggingFace transformers.\"\"\"\n",
    "            embeddings = []\n",
    "            \n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i+batch_size]\n",
    "                \n",
    "                # Tokenize\n",
    "                inputs = tokenizer(batch, padding=True, truncation=True, \n",
    "                                 return_tensors='pt', max_length=512)\n",
    "                \n",
    "                # Get embeddings\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                    # Mean pooling\n",
    "                    batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "                    embeddings.extend(batch_embeddings.numpy())\n",
    "                \n",
    "                print(f\"  Processed {min(i+batch_size, len(texts))}/{len(texts)} tickets\")\n",
    "            \n",
    "            return np.array(embeddings)\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully\")\n",
    "        \n",
    "        # Encode texts\n",
    "        print(f\"Encoding {len(df_unclassified)} ticket descriptions...\")\n",
    "        texts = df_unclassified['prepared_text'].tolist()\n",
    "        \n",
    "        embeddings = encode_texts(texts)\n",
    "        print(f\"‚úÖ Generated embeddings with shape: {embeddings.shape}\")\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings = embeddings / norms\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå HuggingFace encoding failed: {e}\")\n",
    "        transformers_available = False\n",
    "        \n",
    "elif sentence_transformers_available:\n",
    "    print(\"üöÄ Using sentence-transformers (if working)\")\n",
    "    # Previous sentence-transformers code would go here\n",
    "    print(\"‚ö†Ô∏è Sentence-transformers had scipy issues\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Advanced transformers not available, will use simple analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformers_available:\n",
    "    print(\"üîç Applying custom DBSCAN clustering...\")\n",
    "    \n",
    "    # Calculate similarity matrix using our custom function\n",
    "    similarity_matrix = compute_cosine_similarity(embeddings)\n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "    \n",
    "    # Try different DBSCAN parameters\n",
    "    eps_values = [0.2, 0.3, 0.4]\n",
    "    min_samples_values = [3, 5]\n",
    "    \n",
    "    best_params = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            labels = custom_dbscan(distance_matrix, eps=eps, min_samples=min_samples)\n",
    "            \n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "            \n",
    "            if n_clusters > 0:\n",
    "                score = n_clusters * (1 - n_noise / len(labels))\n",
    "                print(f\"  eps={eps}, min_samples={min_samples}: {n_clusters} clusters, {n_noise} noise ({n_noise/len(labels):.1%})\")\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = (eps, min_samples, labels)\n",
    "    \n",
    "    if best_params:\n",
    "        eps, min_samples, labels = best_params\n",
    "        print(f\"\\\\n‚úÖ Best parameters: eps={eps}, min_samples={min_samples}\")\n",
    "        \n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        print(f\"üìä Results: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(labels):.1%})\")\n",
    "        \n",
    "        # Add cluster labels to dataframe\n",
    "        df_unclassified['cluster_label'] = labels\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No good clustering parameters found\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Advanced clustering not available, will use simple grouping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4B: Simple Semantic Analysis (Fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (sentence_transformers_available and sklearn_available):\n",
    "    print(\"üîß Using simple semantic analysis based on keyword similarity\")\n",
    "    \n",
    "    # Extract keywords from each ticket\n",
    "    def extract_keywords(text):\n",
    "        \"\"\"Extract keywords from text.\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Simple keyword extraction\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        words = text.split()\n",
    "        \n",
    "        # Filter out common words\n",
    "        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'user', 'system', 'issue', 'problem', 'help', 'support', 'ticket', 'request'}\n",
    "        \n",
    "        keywords = [word for word in words if len(word) > 2 and word not in stop_words]\n",
    "        return set(keywords)\n",
    "    \n",
    "    df_unclassified['keywords'] = df_unclassified['prepared_text'].apply(extract_keywords)\n",
    "    \n",
    "    # Simple clustering based on keyword similarity\n",
    "    print(\"üîç Finding similar tickets using keyword matching...\")\n",
    "    \n",
    "    similarity_threshold = 0.3\n",
    "    clusters = []\n",
    "    processed = set()\n",
    "    \n",
    "    for i, row in df_unclassified.iterrows():\n",
    "        if i in processed:\n",
    "            continue\n",
    "            \n",
    "        cluster = [i]\n",
    "        keywords_i = row['keywords']\n",
    "        \n",
    "        # Find similar tickets\n",
    "        for j, other_row in df_unclassified.iterrows():\n",
    "            if j <= i or j in processed:\n",
    "                continue\n",
    "                \n",
    "            keywords_j = other_row['keywords']\n",
    "            similarity = simple_jaccard_similarity(keywords_i, keywords_j)\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                cluster.append(j)\n",
    "        \n",
    "        if len(cluster) > 1:  # Only keep clusters with multiple tickets\n",
    "            clusters.append(cluster)\n",
    "            processed.update(cluster)\n",
    "    \n",
    "    # Assign cluster labels\n",
    "    labels = [-1] * len(df_unclassified)  # -1 means noise/unassigned\n",
    "    \n",
    "    for cluster_id, cluster_indices in enumerate(clusters):\n",
    "        for idx in cluster_indices:\n",
    "            labels[idx] = cluster_id\n",
    "    \n",
    "    df_unclassified['cluster_label'] = labels\n",
    "    \n",
    "    n_clusters = len(clusters)\n",
    "    n_noise = labels.count(-1)\n",
    "    print(f\"‚úÖ Simple clustering complete: {n_clusters} clusters, {n_noise} unassigned tickets\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Using advanced clustering results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Semantic Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "print(\"üîç ANALYZING SEMANTIC CLUSTERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "unique_labels = set(df_unclassified['cluster_label'])\n",
    "if -1 in unique_labels:\n",
    "    unique_labels.remove(-1)\n",
    "\n",
    "clusters_analysis = {}\n",
    "\n",
    "# Analyze each cluster\n",
    "for cluster_id in sorted(unique_labels):\n",
    "    cluster_tickets = df_unclassified[df_unclassified['cluster_label'] == cluster_id]\n",
    "    \n",
    "    if len(cluster_tickets) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get cluster information\n",
    "    cluster_info = {\n",
    "        'size': len(cluster_tickets),\n",
    "        'categories': cluster_tickets['category'].value_counts().to_dict(),\n",
    "        'sample_descriptions': cluster_tickets['short_description'].head(5).tolist(),\n",
    "        'ticket_indices': cluster_tickets['ticket_index'].tolist()\n",
    "    }\n",
    "    \n",
    "    # Extract common keywords if available\n",
    "    if 'keywords' in cluster_tickets.columns:\n",
    "        all_keywords = []\n",
    "        for keywords in cluster_tickets.get('keywords', []):\n",
    "            if isinstance(keywords, (set, list)):\n",
    "                all_keywords.extend(list(keywords))\n",
    "        \n",
    "        if all_keywords:\n",
    "            cluster_info['common_keywords'] = dict(Counter(all_keywords).most_common(10))\n",
    "    \n",
    "    clusters_analysis[f\"cluster_{cluster_id}\"] = cluster_info\n",
    "\n",
    "# Handle noise points\n",
    "noise_tickets = df_unclassified[df_unclassified['cluster_label'] == -1]\n",
    "if len(noise_tickets) > 0:\n",
    "    clusters_analysis['noise'] = {\n",
    "        'size': len(noise_tickets),\n",
    "        'categories': noise_tickets['category'].value_counts().to_dict(),\n",
    "        'sample_descriptions': noise_tickets['short_description'].head(10).tolist(),\n",
    "        'note': 'These tickets could not be grouped semantically - may represent unique issues'\n",
    "    }\n",
    "\n",
    "print(f\"üìä CLUSTERING STATISTICS:\")\n",
    "print(f\"   Total clusters: {len(unique_labels)}\")\n",
    "print(f\"   Tickets in clusters: {len(df_unclassified[df_unclassified['cluster_label'] != -1])}\")\n",
    "print(f\"   Noise/unique tickets: {len(noise_tickets)}\")\n",
    "print(f\"   Clustering rate: {(len(df_unclassified[df_unclassified['cluster_label'] != -1]) / len(df_unclassified)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top clusters\n",
    "print(\"\\nüéØ TOP SEMANTIC CLUSTERS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "semantic_clusters = {k: v for k, v in clusters_analysis.items() if k != 'noise'}\n",
    "sorted_clusters = sorted(semantic_clusters.items(), key=lambda x: x[1]['size'], reverse=True)\n",
    "\n",
    "for i, (cluster_name, cluster_info) in enumerate(sorted_clusters[:15], 1):\n",
    "    print(f\"\\n{i:2d}. {cluster_name.upper()}: {cluster_info['size']} tickets\")\n",
    "    print(f\"     Categories: {list(cluster_info['categories'].keys())}\")\n",
    "    print(f\"     Sample descriptions:\")\n",
    "    for j, desc in enumerate(cluster_info['sample_descriptions'][:3], 1):\n",
    "        print(f\"       {j}. '{desc}'\")\n",
    "    \n",
    "    if 'common_keywords' in cluster_info and cluster_info['common_keywords']:\n",
    "        top_keywords = list(cluster_info['common_keywords'].keys())[:5]\n",
    "        print(f\"     Common keywords: {top_keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Visualization (If Possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster size visualization\n",
    "if len(semantic_clusters) > 0:\n",
    "    cluster_sizes = [info['size'] for info in semantic_clusters.values()]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(cluster_sizes, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Cluster Size')\n",
    "    plt.ylabel('Number of Clusters')\n",
    "    plt.title('Distribution of Cluster Sizes')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top clusters bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    top_10_clusters = sorted_clusters[:10]\n",
    "    cluster_names = [f\"Cluster {i+1}\" for i in range(len(top_10_clusters))]\n",
    "    sizes = [info['size'] for _, info in top_10_clusters]\n",
    "    \n",
    "    plt.bar(cluster_names, sizes, color='lightcoral', alpha=0.7)\n",
    "    plt.xlabel('Clusters')\n",
    "    plt.ylabel('Number of Tickets')\n",
    "    plt.title('Top 10 Largest Clusters')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualization created\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No clusters found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = Path('../outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save cluster analysis\n",
    "semantic_results = {\n",
    "    'analysis_timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'methodology': {\n",
    "        'approach': 'Advanced' if sentence_transformers_available else 'Simple keyword-based',\n",
    "        'model': 'all-MiniLM-L6-v2' if sentence_transformers_available else 'Keyword similarity',\n",
    "        'clustering': 'DBSCAN' if sklearn_available else 'Simple grouping'\n",
    "    },\n",
    "    'statistics': {\n",
    "        'total_tickets': len(df_unclassified),\n",
    "        'total_clusters': len(unique_labels),\n",
    "        'tickets_clustered': len(df_unclassified[df_unclassified['cluster_label'] != -1]),\n",
    "        'noise_points': len(noise_tickets),\n",
    "        'clustering_rate': (len(df_unclassified[df_unclassified['cluster_label'] != -1]) / len(df_unclassified)) * 100\n",
    "    },\n",
    "    'clusters': clusters_analysis\n",
    "}\n",
    "\n",
    "# Save JSON results\n",
    "results_file = output_dir / 'semantic_analysis_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(semantic_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_file}\")\n",
    "\n",
    "# Save cluster assignments CSV\n",
    "cluster_assignments = df_unclassified[['ticket_index', 'category', 'short_description', 'cluster_label']].copy()\n",
    "assignments_file = output_dir / 'semantic_cluster_assignments.csv'\n",
    "cluster_assignments.to_csv(assignments_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Cluster assignments saved to: {assignments_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SEMANTIC ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = semantic_results['statistics']\n",
    "print(f\"üìä FINAL RESULTS:\")\n",
    "print(f\"   Total unclassified tickets processed: {stats['total_tickets']:,}\")\n",
    "print(f\"   Semantic clusters found: {stats['total_clusters']}\")\n",
    "print(f\"   Tickets successfully clustered: {stats['tickets_clustered']:,} ({stats['clustering_rate']:.1f}%)\")\n",
    "print(f\"   Unique/noise tickets: {stats['noise_points']:,}\")\n",
    "\n",
    "print(f\"\\nüéØ METHODOLOGY USED:\")\n",
    "method = semantic_results['methodology']\n",
    "print(f\"   Approach: {method['approach']}\")\n",
    "print(f\"   Model: {method['model']}\")\n",
    "print(f\"   Clustering: {method['clustering']}\")\n",
    "\n",
    "print(f\"\\nüîÑ NEXT STEPS:\")\n",
    "print(f\"   1. Review semantic clusters to validate problem groupings\")\n",
    "print(f\"   2. Create core problem statements for each cluster\")\n",
    "print(f\"   3. Combine with hardcoded classifications (639 tickets) for final taxonomy\")\n",
    "print(f\"   4. Calculate total automation potential across all classifications\")\n",
    "\n",
    "# Calculate combined classification rate\n",
    "hardcoded_classified = 639  # From previous results\n",
    "semantic_clustered = stats['tickets_clustered']\n",
    "total_tickets = 3847  # Total consolidated tickets\n",
    "combined_rate = ((hardcoded_classified + semantic_clustered) / total_tickets) * 100\n",
    "\n",
    "print(f\"\\nüèÜ COMBINED CLASSIFICATION SUCCESS:\")\n",
    "print(f\"   Hardcoded rules: {hardcoded_classified:,} tickets (16.6%)\")\n",
    "print(f\"   Semantic clustering: {semantic_clustered:,} tickets ({stats['clustering_rate']:.1f}%)\")\n",
    "print(f\"   Total classified: {hardcoded_classified + semantic_clustered:,} tickets ({combined_rate:.1f}%)\")\n",
    "print(f\"   Remaining unique: {total_tickets - hardcoded_classified - semantic_clustered:,} tickets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
